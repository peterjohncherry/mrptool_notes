In most cases the ranges of the molecular orbital indexes can be split into a
number of different subranges, which enable us to define each of the operator
tensors as the sum of a number of different blocks, each one associated with a
different combination of these subranges. Decomposing the tensors in this
manner reduces the maximum size of the data structure the computer has to deal
with at any one time. Block decomposition also facilitates the implementation
of symmetry, as many blocks are either equivalent to one another or connected
via some simple transformation. This is particularly relevant in the
relativistic case, where Kramers symmetry can potentially be used to ensure
that the size of the blocks we deal with is no larger than those encountered in
the non-relativistic case.\\

\noindent Before proceeding further it is worth mentioning that the design of
the basic tensor operator object is heavily informed by the functionality
required of the algebraic manipulation routines. For these manipulation 
routines to make sense it is necessary to be very precise about the 
definition of the tensors. Consequently, some of the exposition will 
seem tedious, and the design choices unnatural, at least until the algorithms in the main algebraic
manipulator are discussed.\\ 

\noindent The design of this object is easiest explained by example. Consider an operator; $\hat{Q}$,
which may be written in second quantized form as
\begin{equation}
\hat{Q} =  \sum_{i}^{R^{i}}\sum_{j}^{R^{j}}\sum_{k}^{R^{k}}\sum_{l}^{R^{l}} \hat{a}^{\dagger}_{i} \hat{a}^{\dagger}_{j} \hat{a}_{k} \hat{a}_{l} Q_{ijkl}.
\label{eqn:Q_def}
\end{equation}
\noindent  The coefficients,
$Q_{ijkl}$, can be thought of as elements of a tensor $\mathbf{Q}$, which is
the representation of the operator $\hat{Q}$ in the molecular orbital basis.
I shall refer to such tensors as operator tensors. The $R^{i}$ is just the set
of values over which index $i$ runs (note the range is index specific). This
$R^{i}$ can be broken down into $N_{block}$ disjoint subsets or sub ranges, i.e.,
\begin{equation}
R^{i} = r_{1}^{i} \cup r_{2}^{i} \cup r_{3}^{i} \cup ... = \bigcup^{N_{block}}_{\mu} r_{\mu}^{i}
\end{equation}
\noindent A range block, $\mathbf{B}$, specifies the combinations of indexes which lie within some specified ranges. For example, the (ordered) set
of sub ranges, $\{ r_{\mu},\text{\ } r_{\nu}, \text{\ }r_{\xi}, \text{\ } r_{\chi} \}$, defines range block $B^{\mu\nu\xi\chi}$, which specifies the set
of ranges.
\begin{equation}
B^{\mu\nu\xi\chi} :=  \{ \{i,j,k,l\}  \text{\ }|  \text{\ }i \in \ r_{\mu}^{i} , \text{\ } j \in r_{\nu}^{j},  \text{\ } k \in r_{\xi}^{k},  \text{\ }l\in r^{l}_{\chi} \}.
\end{equation}
The components, $ Q_{ijkl}^{b}$, of a tensor block, $\mathbf{Q}^{b}$ are those blocks which satisfy
\begin{equation}
Q_{ijkl} \in \mathbf{Q}^{b} \text{ \ \ iff \ \ }  \{ i, j, k, l \} \in b   
\end{equation}
using this notation we can rewrite (\ref{eqn:Q_def}) as 
\begin{equation}
\hat{Q} =  \sum_{b}^{\{B\}} \sum_{\{i,j,k,l\}}^{b }
\hat{a}^{\dagger}_{i} \hat{a}^{\dagger}_{j} \hat{a}_{k} \hat{a}_{l} Q_{ijkl}.
\label{eqn:Q_block_def}
\end{equation}
\noindent 

\noindent For each operator $\hat{Q}$, we store a single \emph{TensOp} object, which corresponding to
a molecular orbital tensor $\mathbf{Q}$. This object contains a list of blocks, $Q^{b}$, information about which is stored
in  \emph{CtrTensorPart} objects. To facilitate the exploitation of symmetry and sparsity, the task list generated by the program 
only contains instructions referring to tensor blocks, never the full operators themselves.

\section { Block symmetry and sparsity }

Some blocks of the tensor can be obtained by transforming other blocks. How the program exploits this is best explained
through a series of examples.\\ 

\noindent Consider two tensor blocks $Q^{b1}$ and $Q^{b2}$ between which there is the relation
\begin{equation}
Q^{b1}_{ijkl} = Q^{b2}_{ijlk}\rho,
\label{eqn:Qsymm}
\end{equation}
where $b1 = B^{\xi\xi\chi\xi }$, $b2 := B^{\xi\xi\xi\chi }$, and $\rho$ is some constant factor. This is a special case were the ranges $r^{\xi}$ and $r^{\chi}$ have
the same length:
\begin{equation}
\text{card}(r^{\xi}) = \text{card}(r^{\chi}).
\end{equation}
This case regularly occurs when dealing with spin symmetry. Here an element of block $Q^{b1}$ is equal to an element of block $Q^{b2}$ with the last two indexes interchanged.
Therefore, we do not need to store both blocks; operations on $Q^{b2}$ are equivalent to operations on $Q^{b1}$ preceded by a transformation.\\

\noindent Now consider a third, unrelated tensor block $R^{b3}$, where $b3 := \{\xi, \xi, \chi, \xi, \nu, \nu \}$. 
This tensor block is large, and $r^{\nu}$ has far greater extent than $r^{\chi}$, i.e.,
\begin{equation}
\text{card}(r^{\nu}) \gg \text{card}(r^{\chi})
\end{equation}

\noindent For example, consider calculation of elements of a tensor $\mathbf{X}$ from contractions between tensors $\mathbf{Q}$ and $\mathbf{R}$;
\begin{equation}
X_{mn} = \sum_{ijkl} Q^{b1}_{ijkl} R_{ijklmn}+ \sum_{ijkl} R^{b3}_{ijlkmn}Q^{b2}_{ijkl}.
\label{eqn:Xexample}
\end{equation} 
Note that the $l$ and $k$ indexes on $R$ have been interchanged. \\

\noindent A straight forward way to calculate this is by the following two tasks:
\begin{equation*}
\text{task A1\ : \ \ \  \ } X_{mn} = \sum_{ijkl}  Q^{b1}_{ijkl}R^{b3}_{ijklmn},
\end{equation*}                                                             
\begin{equation*}                                                           
\text{task A2\ : \ \ \  \ } X_{mn} += \sum_{ijkl} Q^{b2}_{ijlk}R^{b3}_{ijklmn}.
\end{equation*}
where the indexes on $Q^{b2}$ have been swapped to avoid the swapping the indexes on $R$.
Applying symmetry we end up with three tasks, but don't need to store $Q^{b2}$, i.e., 
\begin{equation*}
\text{task B1\ : \ \ \  \ } X_{mn} = \sum_{ijkl}  Q^{b1}_{ijkl}R^{b3}_{ijklmn}
\end{equation*}                                                             
\begin{equation*}                                                           
\text{task B2\ : \ \ \  \ } Q^{b2}_{ijkl} = - (\text{swap}(k,l)[ Q^{b1}_{ijkl}])
\end{equation*}
\begin{equation*}                                                           
\text{task B3\ : \ \ \  \ } X_{mn} += \sum_{ijkl} Q^{b2}_{ijkl}R^{b3}_{ijlkmn}
\end{equation*}
However, this saves on storage, not computation time. A better approach would be
\begin{equation*}                                                           
\text{task D1\ : \ \ \  \ } A^{b1}_{ijkl} =  Q^{b1}_{ijkl} + \rho(\text{swap}(k,l)[Q^{b1}_{ijkl}] ) 
\end{equation*}
\begin{equation*}                                                           
\text{task D2\ : \ \ \  \ } X_{mn} = \sum_{ijkl} A^{b1}_{ijkl}R^{b3}_{ijklmn}.
\end{equation*}
This could be improved even further to
\begin{equation*}                                                           
\text{task E1\ : \ \ \  \ } X_{mn} = \sum_{ijkl} (1+\rho) Q^{b1}_{ijkl}R^{b3}_{ijklmn}.
\end{equation*}
\noindent Getting to this final task list, containing just task E1, can be
extremely beneficial, not only because we can often replace several transposes
and summations with a single scalar multiplication, but also because if $\rho =
-1$, then no computation needs to be performed at all.\\  

\noindent To go from tasks $A1$ and $A2$ to tasks $D1$ and $D2$ all that is
required is that whenever the program wants the data for tensor block $Q^{b2}$,
it instead fetches the data for $Q^{b2}$ and performs the appropriate
transformation.\\

\noindent Ensuring that tasks $D1$ and $D2$ are replaced by task
$E1$ is a bit more involved, as it requires that symmetry is accounted 
for during the construction of the task list. The full details of this
will wait until the next section, but it the basic ideas governing 
how symmetry is handled can be discussed now.

\noindent The set of all the blocks of a tensor, $\ss$, can be generated by applying
the appropriate transformations to some subset of these blocks;
\begin{equation*}
S : \tilde{B} \rightarrow B  \text { \ \ \ } \tilde{B} \subset B
\end{equation*}
\begin{equation*}
   \tilde{b} \mapsto b   \text { \ \ \ } \tilde b\in \tilde{B} \text{ \ \ \ } b \in B
\end{equation*}
\begin{equation}
   b = t \tilde{b} 
\end{equation}
Only the data corresponding to the minimal set, $\tilde{b}$, of blocks is stored. 
Each \emph{TensOp} object contains a list of \emph{rangeblock} objects, which
define this mapping between the  block (in the full set, $B$), and
the master range block (from the minimal set, $\tilde{B}$). Loosely speaking,
the algebraic manipulation routines just replace all appearances of the original 
blocks with master blocks, plus some transformation.\\ 

\noindent It is important that the minimal set of blocks is consistent, so
that if the same operator appears at multiple places in the same expression, only
subject to different transformations, no unnecessary blocks are stored. For
example, $\mathbf{\lambda}$, should correspond to the same minimal
set as $\lambda^{T}$. Accordingly, the transformations routines are 
written such that multiple transformations can be applied to the same block; in this case
there is a transpose operation, followed by the transformation into the minimal set 
associated with $\lambda$. Note that actual data  corresponding to the tensor representations
(i.e., the large, many index arrays) is not being transformed, only the algebraic objects 
which represent this data using the task list construction.

\noindent It is important to note that the tensor blocks themselves are 

\section{ MultiTensOp } 
\noindent Another important matter is combinations of multiple operators, e.g.,
\begin{equation}
\langle I | \hat{\lambda}\hat{H}\hat{T} | J \rangle .
\end{equation}
The symmetry operations associated with these operators are combined; the increase in the saving
associated with block symmetry is multiplicative, and so increases steeply with
the number of concatenated operators. Furthermore, there is a canonical
operator ordering, so as to avoid storing multiple terms which differ only by some transpose, 
e.g., 
\begin{equation}
\lambda^{b1}H^{b1}T^{b1} = T^{b1}H^{b1}\lambda^{b1}. 
\label{eqn:tens_block_combined}
\end{equation}
\section{CtrTensOp and CtrMultiTensOp}
The task list generator often produces terms which are formed from contractions between indexes either
on the same,
\begin{equation*}
H^{(b1,jk)}_{il} = \sum_{jk} H^{b1}_{ijkl}\delta{jk},
\end{equation*}
or different tensors, 
\begin{equation}
[HT]^{(b1,b2,jz,kw)}_{ilxy} = \sum_{jz} \sum_{kw} H^{b1}_{ijkl}T^{b2}_{wxyz}\delta_{jz}\delta_{kw}.
\label{eqn:cmtp_example}
\end{equation}
These correspond to the CtrTensOp object; note that once the contraction between $H$ and $T$ has occurred, they
are usually inextricable (short of performing some potentially expensive decomposition), and so 
are treated just like a normal tensor block. In terms involving two
tensors which are not contracted all operations on the product tensor should be decomposed into operations of these 
two individual tensors\footnote{this is currently not done everywhere in the code} (note that decomposing the
operation does not, in the cases relevant here, carry the same cost as decomposing the tensors).\\

\noindent It is vital that all transformations are taken account of before the task list of required contractions
is generated. Hence, the RHS in (\ref{eqn:cmtp_example}) above may well be replaced with something like this in
the algebraic task list
\begin{equation}
[HT]^{(b1,b2,jz,kw)}_{ilxy} \mapsto \sum_{ix} \sum_{ly} \rho(H^{\tilde{b1}}_{jilk}T^{\tilde{b2}}_{yzwx}\delta_{ix}\delta_{ly}).
\label{eqn:cmtp_example_symm}
\end{equation}
\noindent Here the original blocks,  $b1$ and $b2$, have been replaced with blocks, $\tilde{b1}$ and $\tilde{b2}$,
from the minimal set, the indexes reshuffled, and a factor $\rho$ associated with the various transformations has appeared. 

\section{ Contraction procedure } 
\noindent In most contexts, the tensor contractions are performed using a recursive sequence of binary contractions. For example,
\begin{equation}
A_{kwzm} = 
[\tilde{B}\tilde{C}\tilde{D}]^{(jy,lx,no,pi)}_{kwzm} = \sum_{jy}\sum_{no}\sum_{lx}\sum_{ip} B_{ijkl}C_{mnop}D_{wxyz} \delta_{jy} \delta_{no} \delta_{lx}\delta_{ip}
\end{equation}
The far LHS is just the output tensor. 
The term $[\tilde{B}\tilde{C}\tilde{D}]^{(jy,lx,no,pi)}_{kwzm}$, indicates that the tensor blocks
$\tilde{B}$, $\tilde{C}$, and $\tilde{D}$ have been contracted over the indexes in the superscript, whilst
the indexes in the subscript remain uncontracted. The tilde indicates that these are
all master blocks (a choice made for the sake of simplifying the example). 
Would be obtained  through the following sequence of operations:
\begin{equation}
\text{Task1 \ : \ \ }
[\tilde{C}]^{(no)}_{mp} = \sum_{no} \tilde{C}_{mnop}\delta_{no}.
\label{eqn:ctr_list_t1}
\end{equation}
\begin{equation}
\text{Task2 \ : \ \ }
[\tilde{B}\tilde{C}]^{(ip,no)}_{jklm} = \sum_{pi} \tilde{B}_{ijkl}[\tilde{C}]_{mp}^{(no)}\delta_{ip}.
\label{eqn:ctr_list_t2}
\end{equation}
\begin{equation}
\text{Task3 \ : \ \ }
[\tilde{B}\tilde{C}\tilde{D}]^{(ip,no,jy)}_{klmwxq} = \sum_{jy} [\tilde{B}\tilde{C}]^{(ip,no)}_{jklm}
 \tilde{D}_{wxqy}\delta_{jy}.
\label{eqn:ctr_list_t3}
\end{equation}
\begin{equation}
\text{Task4 \ : \ \ }
[\tilde{B}\tilde{C}\tilde{D}]^{(ip,no,jy,lx)}_{kmwq} = \sum_{lx} [\tilde{B}\tilde{C}]^{(ip,no)}_{klmwxq} D_{wxqy}\delta_{lx}.
\label{eqn:ctr_list_t4}
\end{equation}
A task list for performing the necessary contractions, transpositions, multiplications, etc.,
is generated from the information stored in the CtrMultiTensOp corresponding to $[BCD]^{(jy,lx,no,pi)}_{kwzm}$.
The design of the algorithm for generating the task list is motivated by five basic principles: 
\begin{itemize}
\item Avoid storing or multiplying large tensor blocks. The size of a tensor block
is assumed to be directly correlated with its order. 
\item Identify if an intermediate result in one task list may be reused in another.
\item Task lists should be comparable; if two different task lists will produce results which will cancel, or which
can be connected by symmetry, this should be recognized and taken advantage of.
\item Every step of the task list contains information about what inputs it needs, and what outputs it produces.
\item The algebraic task list is purely symbolic, and the tasks themselves should not be dependent on
how the tensor data is stored.
\end{itemize}
\noindent These principals lead to the following, more specific rules:\\

\noindent \textbf{Rule 1} : Contractions between indexes on the same tensor are
performed first; this will help reduce the rank of the largest tensor to be stored to
a minimum. For example, if the first two tasks, (\ref{eqn:ctr_list_t1})  and (\ref{eqn:ctr_list_t2}), were reversed;
\begin{equation*}
\text{Task1' \ : \ \ }
[BC]^{(ip)}_{jklmno} = \sum_{pi} B_{ijkl}[C]_{mp}^{(no)}\delta_{ip}.
\end{equation*}
\begin{equation*}
\text{Task2' \ : \ \ }
[BC]^{(ip,no)}_{jklm} = \sum_{pi} [BC]^{(ip)}_{jklmno}\delta_{no}.
\end{equation*}
the most expensive operation will be contraction of two  4-index tensors, whereas in original formulation, the most expensive is contraction of a 4-index tensor with a 2-index tensor.\\

\noindent \textbf{Rule 2} : The contraction list is generated in accordance with an
ordering principle, such that the order in which the contractions are performed
is always the same, e.g.,  the task lists for $[\tilde{B}\tilde{C}\tilde{D}]^{(jy,lx,no,pi)}_{kwzm}$
and $[\tilde{B}\tilde{C}\tilde{D}]^{(kw,lz,no,pi)}_{jxym}$ will always perform the contraction over
$(n,o)$ first, followed by the contraction over $(i,p)$. This has two 
advantages; first the "intermediate" tensor (intermediate in the sense 
it is not the result we seek), $[\tilde{B}\tilde{C}]^{(ip,no)}_{jklm}$ can be reused, and the first two tasks need not be
repeated. Furthermore, this consistent ordering facilitates merging of contraction lists.\\

\noindent \textbf{Rule 3} : A canonical ordering is defined for the indexes and tensors, and
any transpositions are defined relative to this canonical order. For example, tensor
$B$ always appears to the left of $C$, and index $i$ always appears to the left
of $j$. This is also to facilitate merging of task lists.\\

\noindent \textbf{Rule 4} : Each "intermediate" tensor has
use count, which defines the number of remaining tasks which use this tensor (the
same intermediate result may be used in many different task lists). When there is 
no more use for the tensor, it should be deleted\footnote{The count is
implemented, but the deletion is not.}.\\

\noindent \textbf{Rule 5} : The contraction task list is always expressed in terms
of operations on the master blocks.  The
transformations connecting the master blocks to the general blocks have two
components; a multiplicative factor\footnote{Or possibly a complex conjugation,
which reduces to a factor when using real arithmetic.}, and an index
transposition.  Rather than transposing the indexes of the tensor blocks, and
then executing the task list, the indexes over which the contractions are
performed are altered to reflect the index transpositions, and the actual
index transpositions are performed as the final stage of the task list, i.e.,
after all contractions have been performed. To illustrate why
this approach is taken please consider the following example, suppose we would
like to calculate
\begin{equation}
A_{kwzm} = 
[BCD]^{((b1,b2,b3),(jy,lx,mo,kp))}_{inwz} = \sum_{jy}\sum_{no}\sum_{lx}\sum_{ip} B_{ijkl}C_{mnop}D_{wxyz} \delta_{jy} \delta_{no} \delta_{lx}\delta_{ip}
\end{equation}
whilst taking advantage of the following symmetry operations:
\begin{equation*}
B_{ijkl}^{b1} = B_{klij}^{\tilde{b1}}  \text{ \ \ ( 0123 $\rightarrow$ 2301 ) }
\end{equation*}
\begin{equation*}
C_{mnop}^{b2} = C_{nmop}^{\tilde{b2}}  \text{ \ \ ( 0123 $\rightarrow$ 1023 ) }
\end{equation*}
\begin{equation*}
D_{wxyz}^{b3} = D_{zxyw}^{\tilde{b3}}  \text{ \ \ ( 0123 $\rightarrow$ 3120 ) }
\end{equation*}
To simplify the notation, I shall now use $\tilde{D}$ to denote $D^{\tilde{b3}}$. The following tasks would be performed,
\begin{equation*}
\text{Task1 \ : \ \ }
[\tilde{C}]^{(no)}_{mp} = \sum_{no} \tilde{C}_{mnop}\delta_{no}   \text{ \ \ \ $\delta_{mo} \rightarrow \delta_{no}$ }.
\end{equation*}
\begin{equation*}
\text{Task2 \ : \ \ }
[\tilde{B}\tilde{C}]^{(ip,no)}_{jklm} = \sum_{pi} B_{ijkl}[C]_{mp}^{(no)}\delta_{ip} 
\text{ \ \ \ $\delta_{kp} \rightarrow \delta_{ip}$ }.
\end{equation*}
\begin{equation*}
\text{Task3 \ : \ \ }
[\tilde{B}\tilde{C}\tilde{D}]^{(ip,ly,no)}_{klmwxz} = \sum_{ly} [BC]^{(ip,no)}_{jklm} D_{wxyz}\delta_{ly}. 
\text{ \ \ \ $\delta_{lx} \rightarrow \delta_{ly}$ }.
\end{equation*}
\begin{equation*}
\text{Task4 \ : \ \ }
[\tilde{B}\tilde{C}\tilde{D}]^{(ip,ly,mx,no)}_{klwz} = \sum_{mx} [BC]^{(ip,no)}_{klmwxz} D_{wxyz}\delta_{mx}.
\text{ \ \ \ $\delta_{jy} \rightarrow \delta_{mx}$ }.
\end{equation*}
\begin{equation*}
\text{Task5 \ : \ \ }
[BCD]^{(jy,lx,mo,kp)}_{inzw} = swap(2,3) [\tilde{B}\tilde{C}\tilde{D}]^{(ip,ly,mx,no)}_{klwz}
\end{equation*}
where changes to contractions versus those that would be performed on the
untransformed blocks are noted on the right. By waiting until the last
possible moment to apply the index transpositions,
only one index transposition is required; the swapping of the indexes 2 and 3.
Furthermore, it enables the program to identify that the $[\tilde{B}\tilde{C}]^{(ip,no)}_{jklm}$,
obtained in Task 2 (\ref{eqn:ctr_list_t2}) of the previous example task list, can be reused.\\ 
 
\noindent \textbf{Rule 6} : All factors
arising from symmetry transformations are applied after all necessary
contractions have been performed. This keeps the size of the array which needs
to be scaled to a minimum, and also facilitates the tensor reuse and task list comparison described above.
